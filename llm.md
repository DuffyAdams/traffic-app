# san diego traffic watch - llm handoff doc

this markdown file is written to help an llm quickly understand the project structure, the runtime flow, the data model, and the key places to edit safely.

---

## high-level summary

**san diego traffic watch** is a full-stack app that:
1) scrapes chp cad incidents for a specific comm center (`bccc`)
2) fetches per-incident details (including lat/lon) by doing a follow-up postback using `__viewstate`
3) reverse geocodes lat/lon into neighborhood + city (nominatim)
4) generates a static map image for each new incident (`generate_map.py`) and stores it in `traffic-app/maps`
5) stores incidents + likes + comments in a sqlite db (`traffic_data.db`)
6) serves a svelte frontend (built files under `traffic-app/dist`) via flask
7) exposes rest endpoints used by the frontend:
   - list incidents (cursor pagination, filtering)
   - incident stats (counts, breakdowns, chart data)
   - like/unlike
   - comment

the app runs continuously:
- a background scraper thread updates the db and marks incidents active/inactive
- flask serves the api and the frontend

---

## main files described here

### frontend
- `traffic-app/src/App.svelte` (your full ui logic is in this single component right now)
- `traffic-app/src/app.css` (global-ish styles and css variables)

### backend
- `traffic_scraper.py` (scraper + db + flask api + server entrypoint)
- `generate_map.py` (not included here but referenced; it generates the `/maps/*.png` images)

---

## architecture diagram (mental model)

**chp cad site**
- GET incidents page (html table)
- POST back (select a row using viewstate) to get details including coords

⬇

**scraper thread** (`monitor_traffic_data`)
- scrape all current incidents
- for each incident:
  - fetch extra details (lat/lon + details list)
  - reverse geocode (neighborhood/city)
  - if brand-new: generate map png
  - upsert into sqlite (and generate llm summary when needed)
- after scrape: mark any db incidents not in current scrape as `active=0`

⬇

**flask api**
- `/api/incidents` -> incidents for ui feed/table view (cursor paging, filter type, active only)
- `/api/incident_stats` -> counters + chart data + breakdowns
- likes/comments endpoints

⬇

**svelte ui**
- pulls incidents and stats
- supports table/card views, infinite scroll, active-only toggle
- chart.js line chart for activity (hour/day/month/year)
- likes/comments with optimistic updates
- offline detection + toasts
- swipe gestures + pull-to-refresh

---

## data model (sqlite)

database file: `traffic_data.db`

### table: `incidents`
primary key: `(incident_no, date)`

columns (important ones):
- `incident_no` (text)
- `date` (text, `yyyy-mm-dd`)
- `timestamp` (text, `yyyy-mm-dd hh:mm:ss`) - used for ordering and paging
- `city`, `neighborhood`, `location`, `location_desc`
- `type` (standardized in `init_db()` for older rows + during save)
- `details` (json string of list)
- `description` (tweet-length summary generated by openai)
- `latitude`, `longitude`
- `map_filename` (png name generated by `generate_map.py`)
- `likes` (integer)
- `comments` (legacy json column, not actively used as source of truth)
- `active` (integer 0/1) set by monitor loop

### table: `likes`
- `device_uuid` (text)
- `incident_no` (text)
- `timestamp` (text)
primary key: `(device_uuid, incident_no)`

### table: `comments`
- `id` (autoincrement)
- `device_uuid`
- `incident_no`
- `username`
- `comment`
- `timestamp`

note: likes/comments are keyed by `incident_no` only (not `(incident_no, date)`), which means:
- if the same `incident_no` repeats on a different date, likes/comments could collide across days.
- the frontend currently uses `post.id = incident_no`, and `post.compositeId = incident_no + date` mainly for frontend dedupe + keyed rendering.
if you want strict separation by date, you would need to include `date` in likes/comments schema and in endpoints.

---

## backend: traffic_scraper.py walkthrough

### configuration
- `BASE_DIR`, `TARGET_DIR`, `DB_FILE`, `MAP_GENERATOR`
- `SCRAPE_URL` points at chp cad traffic page (bccc)
- cookie name: `traffic_app_uuid` (1 year)
- `TESTMODE` disables live openai calls if set true
- openai client uses `GPT_KEY` env var

### init_db()
creates tables if needed, drops and recreates likes/comments tables, and runs a few data cleanup updates:
- normalizes type names (traffic collision variants, maintenance, road conditions, debris, construction)
- deletes incidents with type `request caltrans notify`

### scraping flow

#### scrape_all_incidents()
1) GET chp page
2) parse html table `gvIncidents`
3) for each row:
   - skip “media log”
   - build `table_data` dict from headers + td cells
   - call `get_incident_details(idx, viewstate)`:
     - POST back with:
       - `__EVENTTARGET=gvIncidents`
       - `__EVENTARGUMENT=Select$<row_index>`
       - plus viewstate values
     - parse response via `extract_traffic_info()`
       - extracts lat/lon pairs
       - extracts details text from `<td colspan="6">...</td>` and strips bracketed tags
       - filters out excluded detail events: unit assigned/enroute/at scene
     - reverse geocode lat/lon -> neighborhood/city (nominatim)
4) each merged incident is returned with:
   - coords
   - details list
   - neighborhood/city (when available)
   - plus a new `Date` and `Timestamp` set at scrape time

important: `scrape_all_incidents()` currently skips incidents that fail to fetch extra details.
that means the system is intentionally “quality-first”: no coords = no record.

#### run_map_generator(merged_data)
- runs `python generate_map.py <lon> <lat> <filename>`
- filename includes incident number + a high-resolution timestamp chunk
- sets:
  - `merged_data["Timestamp"]` (localized la time string)
  - `merged_data["MapFilename"]` (basename of png)

#### save_or_update_incident(data)
- determines `incident_no` and `date`
- standardizes type (traffic collision prefix)
- stores details as json string
- checks if row exists:
  - if exists and `details` changed: regenerate openai description and update details/description and set `active=1`
  - if exists and details unchanged: no-op
  - if new: generate openai description and insert full row with `active=1`

note: description generation is only done on:
- new incident insert
- existing incident where details changed

### continuous monitoring

#### monitor_traffic_data(interval=60)
loop every `interval` seconds:
1) scrape_all_incidents()
2) build `active_incident_ids` from scrape
3) for each incident:
   - if brand new -> generate map
   - save_or_update_incident()
4) mark inactive:
   - `active=0` for any db incident_no not in `active_incident_ids`
5) ping healthchecks:
   - success -> `HEALTHCHECK_URL`
   - failure -> `HEALTHCHECK_URL/fail`

### flask routes

#### GET `/api/incidents`
query params:
- `limit` (default 20)
- `offset` (default 0) (still present but cursor paging is preferred)
- `cursor` (timestamp string). if provided: `timestamp < cursor` to paginate older rows
- `type` (exact match filter)
- `active_only=true` (filters active incidents and requires map_filename not null)
- `date_filter` (currently supports `'daily'` inside `read_incidents`, but frontend uses `day/week/month/year` mostly for stats)

returns incidents, each incident includes `comments` pulled from the `comments` table.

cookie behavior:
- if the cookie isn’t set, server sets a new uuid cookie.

#### GET `/api/incident_stats?date_filter=<day|week|month|year>`
returns:
- `eventsToday` (count where date=today)
- `eventsLastHour` (count where timestamp >= now-1h)
- `eventsActive` (count where active=1 and map_filename not null, optionally constrained to filter)
- `totalIncidents` (total count in range or overall)
- `incidentsByType` (counts in range)
- `topLocations` (top 10 in range)
- `hourlyData` (chart data array)
  - if `day`: 24 hourly buckets (last 24h)
  - if `week`: 7 daily buckets
  - if `month`: 30 daily buckets
  - if `year`: 12 monthly buckets

frontend expects `hourlyData` always and interprets it according to the selected time filter.

#### POST/DELETE `/api/incidents/<incident_id>/like`
- uses cookie uuid (generates if missing)
- POST inserts into likes table and increments incidents.likes
- DELETE removes like row and decrements likes count
returns `{ likes: <int> }`

#### POST `/api/incidents/<incident_id>/comment`
payload:
- `{ username, comment, timestamp }`
enforces:
- max 2 comments per username per incident (not per device_uuid)
returns `{ comments: [...] }`

**bug to notice**: in a couple places cookies are set using `httpy=True` which is a typo and should be `httponly=True`. this affects:
- `/api/incidents/<incident_id>/comment`
- `/api/user/check`
(llm making changes should fix this.)

---

## frontend: App.svelte walkthrough

this component currently contains:
- all state
- all fetching
- all ui rendering (header, stats panel, chart, table view, card view, toasts)
- all styles (a lot of css is embedded in this file)

### major state variables
data + loading:
- `posts` (array of processed incidents)
- `loading` (initial)
- `loadingMore` (pagination)
- `allPostsLoaded` (stop pagination)
- `lastCursor` (timestamp cursor for backend paging)
- `apiCache` (map keyed by url string)

filters/views:
- `selectedType` (filter incidents by type)
- `showActiveOnly` (toggles active filter)
- `condensedView` (table vs card)
- `expandedPostId` (expanded row in table view)

stats + chart:
- `eventsToday`, `eventsLastHour`, `eventsActive`, `totalIncidents`
- `incidentsByType`, `topLocations`
- `showEventCounters` (collapse stats panel)
- `timeFilter` (day/week/month/year)
- `hourlyData` (chart data array)
- chart.js refs: `chartCanvas`, `chartInstance`

ux:
- `toasts` (toast notifications)
- `isOnline` (network status)
- swipe variables and pull-to-refresh variables

### incident fetching
#### fetchIncidents()
- aborts previous request with `AbortController`
- builds url:
  - `/api/incidents?limit=<postsPerPage>`
  - adds `cursor` if `currentPage>1` and `lastCursor`
  - adds `type` if selected
  - adds `active_only=true` if toggled
- checks apiCache
- fetches with retry + exponential backoff
- calls `processIncidents(incidents)`

#### processIncidents(incidents)
- validates array + required fields:
  - `incident_no`, `timestamp`, `map_filename` must exist or it skips the record
- creates `compositeId = incident_no + date` where date comes from localizing timestamp
- dedupes using `seenCompositeKeys` but the key is:
  - `incident_no-timestamp-location`
- maps into frontend `post` shape:
  - `id` = `incident_no`
  - `time` formatted
  - `image` = `/maps/${map_filename}`
  - `active` boolean
  - plus likes/comments, local fields like `showComments`, `newComment`, etc
- updates `lastCursor` using the last incident timestamp from the response
- sets `allPostsLoaded = incidents.length < postsPerPage`

note: the dedupe approach is purely client-side. it prevents repeated entries across pagination and refresh but it is not guaranteed if timestamps change or if the same incident_no repeats.

### stats fetching
#### fetchIncidentStats()
- hits `/api/incident_stats?date_filter=<timeFilter>`
- 30 second cache
- updates counters + `hourlyData`
- limits `incidentsByType` and `topLocations` to top 6 entries

### chart.js
- chart is initialized when:
  - `hourlyData` exists and `chartCanvas` is bound
- `initializeChart()` destroys existing instance before creating a new one
- `updateChart()` updates data/labels and recalculates y step size
- chart uses category x-axis labels computed in `chartLabels` reactive statement

### likes/comments
- `likePost()` does optimistic updates, then calls backend endpoint
- `submitComment()` does optimistic add with a temporary id, then replaces with server response

### ui features
- dark mode toggles a `dark-mode` class on `body`
- infinite scroll uses `window` scroll listener with debounce
- swipe gestures switch views on mobile
- pull-to-refresh triggers incident + stats fetch

---

## css: app.css and embedded styles

you currently have:
- `app.css` with base styles and some global css vars for dark mode
- *massive* embedded `<style>` inside `App.svelte` (this is the majority of styling)

theme model:
- css variables on `body` switch in dark mode:
  - `--primary-color`, `--bg-color`, etc
- many components use these vars for consistent theming

note: there are duplicate definitions (some styles appear both in app.css and in App.svelte). if an llm changes styling, it needs to check precedence:
- embedded component `<style>` will generally win due to bundling order and specificity.

---

## important implementation details + gotchas (llm should know)

### 1) incident identity vs day separation
backend incidents pk is `(incident_no, date)`, but likes/comments endpoints operate on `incident_no` only.
frontend uses:
- `post.id = incident_no` (likes/comments use this)
- `post.compositeId = incident_no-date` (render key and some dedupe)

if you want correct per-day likes/comments, you must redesign endpoints and tables.

### 2) timestamps are stored as text
`timestamp` is text in sqlite but formatted as `yyyy-mm-dd hh:mm:ss`, which sorts correctly lexicographically.
cursor paging uses `timestamp < cursor`. this works as long as timestamps keep consistent formatting.

### 3) “active” logic
active state is driven by the latest scrape:
- if incident_no not present in current scrape => active=0
this is simple and works, but can flicker if chp page briefly fails or returns partial data.

### 4) map generation happens only for new incidents
maps are generated only when `incident_exists` returns false.
if a map generation fails once, that incident may stay without a map, and your frontend currently filters out incidents without `map_filename` in `processIncidents()` (required field).
so a map failure can cause an incident to never appear in ui.

### 5) concurrency and sqlite usage
you use a global `db_lock` around db operations, but there are some indentation issues in the snippet that look accidental:
- in `read_incidents()` and `incident_exists()` the `cur.execute()` appears outside the `with sqlite3.connect(...)` block because of indentation.
if this matches your real file, that will break at runtime.
(if it is only a paste formatting issue, ignore.)

best practice: keep connection + cursor usage inside the same `with` block.

### 6) cookie flags typo
some endpoints set cookies with `httpy=True` which is not valid.
should be `httponly=True`.

### 7) openai model call
`generate_description()` calls:
- model: `gpt-5-nano-2025-08-07`
it creates a single sentence, under 200 chars, emoji allowed, no hashtags/warnings.

if you change this behavior, consider cost, latency, and rate-limits because it runs during scraping.

---

## how to run (expected)
backend:
- `python traffic_scraper.py`
  - initializes db
  - starts scraper thread
  - starts flask server on port 5000

frontend build:
- expected to be built into `traffic-app/dist` and served by flask
- maps are served from `/maps/<filename>` from the `traffic-app/maps` directory

env:
- `.env` should include `GPT_KEY` if you want live summaries
- if `GPT_KEY` missing and your code raises, the service will fail unless you keep the guard commented.

---

## api contracts (what the frontend expects)

### `/api/incidents`
response: array of incidents with at least:
- `incident_no`
- `timestamp`
- `location`
- `type`
- `description`
- `map_filename`
- `active` (0/1)
- `likes` (int)
- `comments` (array of `{ username, comment, timestamp }`)

### `/api/incident_stats?date_filter=...`
response:
- `eventsToday` int
- `eventsLastHour` int
- `eventsActive` int
- `totalIncidents` int
- `incidentsByType` object map type -> count
- `topLocations` object map location -> count
- `hourlyData` array of ints
(frontend uses this for day/week/month/year charts)

---

## safe edit zones (guidance for an llm changing code)

### backend safe zones
- add new filters to `/api/incidents` (example: neighborhood, city) by extending `read_incidents()` conditions.
- improve dedupe/server paging by adding stable ordering and returning `next_cursor`.
- fix cookie typos (`httpy` -> `httponly`).
- refactor likes/comments to include `date` if needed.

### frontend safe zones
- split App.svelte into components (stats panel, incident card, incident table, comments overlay)
- replace client dedupe with a server-provided cursor token
- improve chart label formatting and tooltip titles by timeFilter

### danger zones
- changing `timestamp` format will break ordering and cursor paging
- removing `map_filename` requirement in frontend without updating backend may show map-less items or broken images
- changing incident identity rules can break likes/comments behavior

---

## quick glossary

- **incident_no**: id from chp cad, used as primary id in ui and likes/comments endpoints
- **date**: server date string used as part of pk in incidents table
- **active**: incident currently present in the latest scrape
- **cursor**: timestamp of last incident in the current page, used to request older incidents

---

## known improvements (optional backlog)
- fix sqlite connection indentation issues if present in actual file
- fix cookie typo `httpy` -> `httponly`
- unify `date_filter` naming: frontend uses `day/week/month/year` but `read_incidents()` uses `'daily'`
- return `next_cursor` from `/api/incidents` so frontend doesn’t infer it from last element
- include `date` in likes/comments schema to avoid cross-day collisions
- make map generation failures non-fatal (fallback marker image or allow rendering without map)
- stop dropping likes/comments tables in `init_db()` if you want persistence across restarts

---

## llm instructions for making changes

when modifying this codebase:
1) preserve existing api response shapes unless you also update the svelte consumer
2) do not change timestamp formatting unless you update ordering + cursor logic everywhere
3) if adding filters/pagination changes, ensure the frontend dedupe/caching logic still behaves
4) prefer small, isolated changes with clear reasoning and minimal side effects
5) if refactoring, keep endpoints stable or provide a migration plan

end of handoff doc.
